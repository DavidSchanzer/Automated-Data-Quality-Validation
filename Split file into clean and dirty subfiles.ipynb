{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "120229f2",
   "metadata": {},
   "source": [
    "# Split file into clean subfiles, create dirty files with synthetic errors\n",
    "## Author: David Schanzer\n",
    "## Student ID: 82329622\n",
    "### Subject: 32933 Research Project\n",
    "### Session: Spring 2023\n",
    "### Assessment 1: Prototype Research\n",
    "### Supervisor: Dr Amara Atif\n",
    "### This Jupyter Notebook takes a single large multi-year CSV file that contains \"clean\" radiotherapy treatment data and creates individual files for each month, fortnight and week. For each of these \"clean\" files, it also creates an equivalent \"dirty\" file that contains between 1% and 80% of records with a synthetic error, which can one of six different types of error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3051e",
   "metadata": {},
   "source": [
    "Import all required Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f54138e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numbers\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca06818",
   "metadata": {},
   "source": [
    "**_is_number:_** Define a method to determine whether a string contains a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4652a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(string):\n",
    "    try:\n",
    "        float(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e3f104",
   "metadata": {},
   "source": [
    "**rmtree:_** Define a method to remove all files and folders at and below the given pathname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dae975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmtree(f: Path):\n",
    "    if f.is_file():\n",
    "        f.unlink()\n",
    "    else:\n",
    "        for child in f.iterdir():\n",
    "            rmtree(child)\n",
    "        f.rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d3e85",
   "metadata": {},
   "source": [
    "**create_folder:_** Define a method to create an empty folder at the specified pathname, deleting existing files if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a76d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(baseFolder, folderName):\n",
    "    folderPath = baseFolder + folderName\n",
    "    folderPathObj = Path(folderPath)\n",
    "    if folderPathObj.exists():  # If folder already exists\n",
    "        rmtree(folderPathObj)   # then recursively delete all files and folders\n",
    "\n",
    "    folderPathObj.mkdir()       # Create a new empty folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8806598",
   "metadata": {},
   "source": [
    "**gaussian_noise:_** Define a method to take a numeric value and add \"noise\", varying the value within the given mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fd70918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_noise(value, mean, std):\n",
    "    noise = np.random.normal(mean, std)\n",
    "    value_noisy = float(value) + noise\n",
    "    return value_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8658c12c",
   "metadata": {},
   "source": [
    "**generate_clean_files:_** Define a method to generate all required \"clean\" files from a given dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01a2ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clean_files(df, sliceBy, cleanFileNamePrefix, cleanPath):\n",
    "    # Add columns for year, month and week\n",
    "    df['tSiteStartDateYear'] = df['tSiteStartDate'].dt.year\n",
    "    df['tSiteStartDateMonth'] = df['tSiteStartDate'].dt.month\n",
    "    df['tSiteStartDateWeek'] = df['tSiteStartDate'].apply(lambda x: x.isocalendar()[1])\n",
    "    df['tSiteStartDateFortnight'] = df['tSiteStartDate'].apply(lambda x: np.ceil(x.isocalendar()[1] / 2))\n",
    "\n",
    "    if sliceBy == 'Week':          # Slicing by week\n",
    "        byGroup = df.groupby(['tSiteStartDateYear', 'tSiteStartDateWeek'])\n",
    "    elif sliceBy == 'Fortnight':   # Slicing by fortnight\n",
    "        byGroup = df.groupby(['tSiteStartDateYear', 'tSiteStartDateFortnight'])\n",
    "    else:                          # Slicing by month\n",
    "        byGroup = df.groupby(['tSiteStartDateYear', 'tSiteStartDateMonth'])\n",
    "\n",
    "    iterator = iter(byGroup)       # Create an iterator for the groupby object\n",
    "\n",
    "    # Slice the input file into separate files in the clean folder\n",
    "    index = 0\n",
    "    while index < len(byGroup):\n",
    "        group, frame = next(iterator)   # Return the next item from the iterator\n",
    "        year = group[0]                 # The first element of the tuple is the year\n",
    "        period_number = int(group[1])   # The second element of the tuple is the month or fortnight or week number\n",
    "        cleanFileName = (cleanFileNamePrefix + str(index + 1).zfill(3) + '_' + str(year) + '_' + sliceBy +\n",
    "                         str(period_number).zfill(2) + '.csv')\n",
    "\n",
    "        # Drop the extra columns that we added from this frame before it's saved\n",
    "        frame = frame.drop(['tSiteStartDateYear', 'tSiteStartDateMonth', 'tSiteStartDateFortnight',\n",
    "                            'tSiteStartDateWeek'], axis=1)\n",
    "        frame.sort_values(by=['tSiteStartDate'], inplace=True)      # Sort the frame by tSiteStartDate\n",
    "\n",
    "        # Replace NaN values with 'NULL', as this is how empty cells were indicated in the input file\n",
    "        frame.replace(np.nan, 'NULL', inplace = True)\n",
    "        # Save this frame to a CSV file\n",
    "        frame.to_csv(cleanPath + '/' + cleanFileName, index = False, encoding = 'cp1252')\n",
    "\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e61a8",
   "metadata": {},
   "source": [
    "**injectError:_** Define a method to inject a given error type into a given line of data passed as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7ba6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def injectError(dirtyList, randomErrorType, errorColumn, errorColumn2, meanContinuousNumericValue, typoEveryNthChar):\n",
    "    if randomErrorType == 1:     # Explicit missing values (empty cells)\n",
    "        # Just set this column to NULL since this is how empty columns are set in this dataset\n",
    "        dirtyList[errorColumn] = 'NULL'\n",
    "        \n",
    "    elif randomErrorType == 2:   # Implicit missing values (logically empty, 99999 for numerics or NONE for text)\n",
    "        if is_number(dirtyList[errorColumn]):\n",
    "            dirtyList[errorColumn] = '99999'   # Set numeric columns to 99999 to indicate unknown\n",
    "        else:\n",
    "            dirtyList[errorColumn] = 'NONE'    # Set non-numeric columns to NONE to indicate unknown\n",
    "            \n",
    "    elif randomErrorType == 3:   # Numeric anomalies (unexpectedly high or low numeric values)\n",
    "        newValue = dirtyList[errorColumn]\n",
    "        \n",
    "        if pd.isnull(newValue):\n",
    "            newValue = meanContinuousNumericValue   # Since there is a null in this cell, use this column's mean\n",
    "                                                    # as a starting point (before applying Gaussian noise)\n",
    "            \n",
    "        if is_number(newValue):\n",
    "            randomStDevs = np.random.randint(2,5)   # Choose a random number of standard deviations between 2 and 5\n",
    "            # Apply Gaussian noise with this number of SDs\n",
    "            dirtyList[errorColumn] = str(np.round(gaussian_noise(value = newValue, mean = 0, std = randomStDevs), 1))\n",
    "        \n",
    "    elif randomErrorType == 4:   # Swapped numeric fields (swapping values from one numeric attribute to another)\n",
    "        swapValue               = dirtyList[errorColumn]\n",
    "        dirtyList[errorColumn]  = dirtyList[errorColumn2]\n",
    "        dirtyList[errorColumn2] = swapValue\n",
    "        \n",
    "    elif randomErrorType == 5:   # Swapped textual fields (swapping values from one textual attribute to another)\n",
    "        swapValue               = dirtyList[errorColumn]\n",
    "        dirtyList[errorColumn]  = dirtyList[errorColumn2]\n",
    "        dirtyList[errorColumn2] = swapValue\n",
    "        \n",
    "    elif randomErrorType == 6:   # Typographic errors (randomly replace a fraction of letters in textual attributes\n",
    "                                 # with adjacent letters on a QWERTY keyboard)\n",
    "        # Create dictionary of adjacent keys on a QWERTY keyboard\n",
    "        adjacentLetters = {\n",
    "            '1' : ('2','Q'),\n",
    "            '2' : ('1','3','Q','W'),\n",
    "            '3' : ('2','4','W','E'),\n",
    "            '4' : ('3','5','E','R'),\n",
    "            '5' : ('4','6','R','T'),\n",
    "            '6' : ('5','7','T','Y'),\n",
    "            '7' : ('6','8','Y','U'),\n",
    "            '8' : ('7','9','U','I'),\n",
    "            '9' : ('8','0','I','O'),\n",
    "            '0' : ('9','O','P'),\n",
    "            'Q' : ('1','2','W','A'),\n",
    "            'W' : ('2','3','Q','E','A','S'),\n",
    "            'E' : ('3','4','W','R','S','D'),\n",
    "            'R' : ('4','5','E','T','D','F'),\n",
    "            'T' : ('5','6','R','Y','F','G'),\n",
    "            'Y' : ('6','7','T','U','G','H'),\n",
    "            'U' : ('7','8','Y','I','H','J'),\n",
    "            'I' : ('8','9','U','O','J','K'),\n",
    "            'O' : ('9','0','I','P','K','L'),\n",
    "            'P' : ('0','O','L'),\n",
    "            'A' : ('Q','W','S','Z'),\n",
    "            'S' : ('W','E','A','D','Z','X'),\n",
    "            'D' : ('E','R','S','F','X','C'),\n",
    "            'F' : ('R','T','D','G','C','V'),\n",
    "            'G' : ('T','Y','F','H','V','B'),\n",
    "            'H' : ('Y','U','G','J','B','N'),\n",
    "            'J' : ('U','I','H','K','N','M'),\n",
    "            'K' : ('I','O','J','L','M'),\n",
    "            'L' : ('O','P','K'),\n",
    "            'Z' : ('A','S','X'),\n",
    "            'X' : ('S','D','Z','C'),\n",
    "            'C' : ('D','F','X','V'),\n",
    "            'V' : ('F','G','C','B'),\n",
    "            'B' : ('G','H','V','N'),\n",
    "            'N' : ('H','J','B','M'),\n",
    "            'M' : ('J','K','N')\n",
    "        }\n",
    "        string = dirtyList[errorColumn]\n",
    "        if pd.isnull(string):\n",
    "            string = ''\n",
    "        stringList = list(string)\n",
    "        \n",
    "        for element in range(0, len(stringList), typoEveryNthChar):\n",
    "            char = stringList[element]\n",
    "            \n",
    "            if char.isalpha() or char.isnumeric():      # Only replace from the dictionary if a number or a letter\n",
    "                upperChar = char.upper()                # Retrieve this character and convert to upper-case\n",
    "                replacementCharList = adjacentLetters[upperChar]   # Look up the replacement char in the dictionary\n",
    "                randomListElement = random.randint(0, len(replacementCharList) - 1)   # Random element\n",
    "                replacementChar = replacementCharList[randomListElement]\n",
    "\n",
    "                if not char.isupper():                  # If the original character was lower-case\n",
    "                    replacementChar = replacementChar.lower()   # then make replacement character lower-case too\n",
    "\n",
    "                stringList[element] = replacementChar   # Set the replacement character in the string\n",
    "                \n",
    "        dirtyList[errorColumn] = \"\".join(stringList)    # Replace the string in the dirtyList element\n",
    "    else:\n",
    "        print('Error: randomErrorType set to ' + str(randomErrorType))\n",
    "\n",
    "    return dirtyList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ddd72",
   "metadata": {},
   "source": [
    "**generate_dirty_files:_** Define a method to generate all required \"dirty\" files from the \"clean\" files in a given pathname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5a0097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dirty_files(cleanPath, cleanName, dirtyPath, dirtyName, dtypes, parse_dates, errorMagnitudes,\n",
    "                         typoEveryNthChar, continuousNumericMeans, continuousNumericColumns):\n",
    "    \n",
    "    # Iterate through each of the clean files to create the equivalent dirty file\n",
    "    files = Path(cleanPath).glob('Radiotherapy*')\n",
    "    dlist = list(dtypes)\n",
    "    columnNames = ','.join(dlist) + '\\n'\n",
    "    \n",
    "    for file in sorted(files):\n",
    "        # Load the next clean file into a dataframe\n",
    "        cleanDF = pd.read_csv(file, sep = ',', dtype = dtypes, parse_dates = parse_dates, encoding = 'cp1252')\n",
    "\n",
    "        fileErrorMagnitude = errorMagnitudes[random.randint(0,9)]                  # Randomly % of error records\n",
    "        targetErrorCount = math.ceil(fileErrorMagnitude / 100 * cleanDF.shape[0])  # Number of records (rounded up)\n",
    "    \n",
    "        # Inject one of the following types of synthetic error:\n",
    "        # 1. Explicit missing values (empty cells)\n",
    "        # 2. Implicit missing values (cells that are logically empty but contain 99999 for numeric fields or NONE\n",
    "        #    for textual fields)\n",
    "        # 3. Numeric anomalies (unexpectedly high or low numeric values)\n",
    "        # 4. Swapped numeric fields (swapping values from one numeric attribute to another)\n",
    "        # 5. Swapped textual fields (swapping values from one textual attribute to another)\n",
    "        # 6. Typographic errors (randomly replace a fraction of letters in textual attributes with adjacent letters\n",
    "        #    on a QWERTY keyboard)\n",
    "\n",
    "        randomErrorType = random.randint(1,6)           # Choose a random number between 1 and 6\n",
    "        #randomErrorType = 6\n",
    "        numberColumns = cleanDF.shape[1]                # Count the number of columns in the header row\n",
    "        errorColumn2 = np.NaN                           # Initialise 2nd error column to NaN since not always\n",
    "                                                        # needed\n",
    "        meanContinuousNumericValue = np.NaN             # Initialise mean continuuous numeric value to NaN\n",
    "\n",
    "        # List the column elements that are never null, are numeric, non-numeric respectively.\n",
    "        #\n",
    "        # Non-null column numbers below are:\n",
    "        # EpisodeID, PatientID, CaseID, tFirstGivenName, tSurname, tDateOfBirth, tTreatmentStartDate, tSource,\n",
    "        # TreatmentYear, Snapshot, AgeAtTreatment, PatientID.Derived, AddressID.Derived, AddressID, CaseID.Derived,\n",
    "        # RecordID, SiteNameBoost, SiteNameLymphNode\n",
    "        nonNullColumns = [0, 3, 4, 9, 11, 13, 53, 68, 72, 73, 75, 82, 107, 108, 109, 110, 112, 113]\n",
    "        # Non-null numeric column numbers below are:\n",
    "        # EpisodeID, PatientID, CaseID, TreatmentYear, AgeAtTreatment, AddressID, RecordID\n",
    "        nonNullNumericColumns = [0, 3, 4, 72, 75, 108, 110]           \n",
    "        # Non-null non-numeric column numbers below are:\n",
    "        # tFirstGivenName, tSurname, tDateOfBirth, tTreatmentStartDate, tSource, Snapshot, PatientID.Derived,\n",
    "        # AddressID.Derived, CaseID.Derived, SiteNameBoost, SiteNameLymphNode\n",
    "        nonNullNonNumericColumns = [9, 11, 13, 53, 68, 73, 82, 107, 109, 112, 113]                   \n",
    "        \n",
    "        if randomErrorType == 1:     # Explicit missing values (empty cells)\n",
    "            # Select a non-empty column by picking a random integer between 0 and len(nonNullColumns) - 1\n",
    "            errorColumn = nonNullColumns[random.randint(0, len(nonNullColumns) - 1)]\n",
    "                      \n",
    "        elif randomErrorType == 2:   # Implicit missing values (cells that are logically empty but contain 99999\n",
    "                                     # for numeric fields or NONE for textual fields)\n",
    "            # Select a non-empty column by picking a random integer between 0 and len(nonNullColumns) - 1\n",
    "            errorColumn = nonNullColumns[random.randint(0, len(nonNullColumns) - 1)]\n",
    "                    \n",
    "        elif randomErrorType == 3:   # Numeric anomalies (unexpectedly high or low numeric values)\n",
    "            # Select a continuous numeric column by picking a random integer between 0 and\n",
    "            # len(continuousNumericColumns) - 1\n",
    "            errorColumn = continuousNumericColumns[random.randint(0, len(continuousNumericColumns) - 1)]\n",
    "            meanContinuousNumericValue = continuousNumericMeans[str(errorColumn)]\n",
    "                    \n",
    "        elif randomErrorType == 4:   # Swapped numeric fields (swapping values from one numeric attribute to\n",
    "                                     # another)\n",
    "            # Select a numeric column by picking a random integer between 0 and len(nonNullNumericColumns) - 1\n",
    "            errorColumn = nonNullNumericColumns[random.randint(0, len(nonNullNumericColumns) - 1)]\n",
    "                    \n",
    "            # Find a second numeric column with which we can swap values\n",
    "            while True:\n",
    "                # Pick a random integer between 0 and len(nonNullNumericColumns) - 1\n",
    "                errorColumn2 = nonNullNumericColumns[random.randint(0, len(nonNullNumericColumns) - 1)]\n",
    "                if errorColumn2 != errorColumn:   # Ensure that it's a different column\n",
    "                    break\n",
    "                    \n",
    "        elif randomErrorType == 5:   # Swapped textual fields (swapping values from one textual attribute to\n",
    "                                     # another)\n",
    "            # Select a non-numeric column by picking a random integer between 0 and\n",
    "            # len(nonNullNonNumericColumns) - 1\n",
    "            errorColumn = nonNullNonNumericColumns[random.randint(0, len(nonNullNonNumericColumns) - 1)]\n",
    "                    \n",
    "            # Select a second non-numeric column with which we can swap values\n",
    "            while True:\n",
    "                # Pick a random integer between 0 and len(nonNullNonNumericColumns) - 1\n",
    "                errorColumn2 = nonNullNonNumericColumns[random.randint(0, len(nonNullNonNumericColumns) - 1)]\n",
    "                if errorColumn2 != errorColumn:   # Ensure that it's a different column\n",
    "                    break\n",
    "                    \n",
    "        elif randomErrorType == 6:   # Typographic errors (randomly replace a fraction of letters in textual\n",
    "                                     # attributes with adjacent letters on a QWERTY keyboard)\n",
    "            # Select a non-numeric column by picking a random integer between 0 and\n",
    "            # len(nonNullNonNumericColumns) - 1\n",
    "            errorColumn = nonNullNonNumericColumns[random.randint(0, len(nonNullNonNumericColumns) - 1)]\n",
    "        else:\n",
    "            print('Error: randomErrorType set to ' + str(randomErrorType))\n",
    "\n",
    "        # Start with the dirty file name matching the clean file name (and 'clean' replaced with 'dirty')\n",
    "        dirtyFileName = file.name.replace(cleanName, dirtyName)\n",
    "        \n",
    "        # Add suffixes for the percentage of records in error and the type of error injected into this file\n",
    "        dirtyFileName = dirtyFileName[:-4] + '_' + str(fileErrorMagnitude) + '%_error_' + str(randomErrorType) +\n",
    "                        '_in_'\n",
    "        if randomErrorType == 4 or randomErrorType == 5:\n",
    "            dirtyFileName += 'columns_' + dlist[errorColumn] + '_and_' + dlist[errorColumn2]\n",
    "        else:\n",
    "            dirtyFileName += 'column_' + dlist[errorColumn]\n",
    "        \n",
    "        dirtyFileName += '.csv'\n",
    "        \n",
    "        dirtyFile = open(dirtyPath + '/' + dirtyFileName, 'w')   # Open the next dirty file for writing\n",
    "        dirtyFile.write(columnNames)                             # Write the column names as the first row\n",
    "        errorCount = 0\n",
    "\n",
    "        #debugFileName = file.name.replace(cleanName, 'debug')\n",
    "        #debugFile = open(dirtyPath + '/' + debugFileName, 'w')\n",
    "        for cleanTuple in cleanDF.itertuples():                  # For each row in the clean dataframe\n",
    "            dirtyList = list(cleanTuple[1:])                     # Convert clean named tuple to dirty list, so that\n",
    "                                                                 # it can be updated if needed, and remove index\n",
    "\n",
    "            if errorCount < targetErrorCount:                    # More errors need to be injected\n",
    "                # Inject an error into this line\n",
    "                dirtyList = injectError(dirtyList, randomErrorType, errorColumn, errorColumn2,\n",
    "                                        meanContinuousNumericValue, typoEveryNthChar)\n",
    "                errorCount += 1\n",
    "\n",
    "            dirtyTuple = tuple(dirtyList)                        # Convert list back to tuple\n",
    "            dirtyDF = pd.DataFrame(dirtyTuple)                   # Convert tuple to dataframe\n",
    "            # Convert dataframe to comma-separated string (elements separated by comma and space)\n",
    "            dirtyStr = dirtyDF.to_csv(header = False, index = False, na_rep = 'NULL', encoding = 'cp1252')\n",
    "            # Remove space after comma, time portion of tSiteStartDate, and remove opening and closing square\n",
    "            # brackets and commas\n",
    "            dirtyStr = str(dirtyStr.split('\\r\\n')).replace(\"', '\", \",\").replace(\" 00:00:00\", \"\")[2:-3] + '\\n'\n",
    "            # Replace strings \"', \"\" \"\", \"\" and \"\", '\" with just a comma\n",
    "            dirtyStr = dirtyStr.replace(\"', \\\"\", \",\").replace(\"\\\", \\\"\", \",\").replace(\"\\\", '\", \",\")\n",
    "            dirtyStr = dirtyStr.replace(\"\\\\\", \"\")                # Remove backslashes\n",
    "            #debugFile.write('After: dirtyStr = ' + dirtyStr + '\\n')\n",
    "\n",
    "            dirtyFile.write(dirtyStr)                            # Write the line to the dirty file\n",
    "\n",
    "        dirtyFile.close()\n",
    "        #debugFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dadb7d5",
   "metadata": {},
   "source": [
    "**main:_** Define folders for clean and dirty files, the input file, and other parameters, then generate the clean and dirty files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04e73705",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialise file and folder names\n",
    "    sliceBy             = 'Month'   # Value 'Week' means slice the input data by week number,\n",
    "                                    # 'Fortnight' means slice the input data by fortnight number,\n",
    "                                    # and 'Month' to slice by month\n",
    "    baseFolder          = '<pathname_to_base_folder>'\n",
    "    cleanName           = 'clean.By' + sliceBy\n",
    "    dirtyName           = 'dirty.By' + sliceBy\n",
    "    inputFile           = '<radiotherapy_treatment_filename>.csv'\n",
    "    cleanPath           = baseFolder + cleanName\n",
    "    dirtyPath           = baseFolder + dirtyName\n",
    "    cleanFileNamePrefix = 'Radiotherapy_' + cleanName + '_'\n",
    "    dirtyFileNamePrefix = 'Radiotherapy_' + dirtyName + '_'\n",
    "    typoEveryNthChar    = 5                    # Swap every nth character with one adjacent on the QWERTY keyboard\n",
    "    errorMagnitudes     = [1, 5, 10, 20, 30, 40, 50, 60, 70, 80]   # % of records with a synthetic error in a file\n",
    "    \n",
    "    inputPath = baseFolder + inputFile\n",
    "    inputPathObj = Path(inputPath)\n",
    "    \n",
    "    if inputPathObj.exists():                  # If input file exists\n",
    "        create_folder(baseFolder, cleanName)   # Create a new empty clean folder\n",
    "        create_folder(baseFolder, dirtyName)   # Create a new empty dirty folder\n",
    "\n",
    "        # Define all columns in the input file, specifying to read_csv that all are imported as type \"object\"\n",
    "        dtypes = {\n",
    "            'EpisodeID': 'object',\n",
    "            'tSiteStartDate': 'object',\n",
    "            'tSiteEndDate': 'object',\n",
    "            'PatientID': 'object',\n",
    "            'CaseID': 'object',\n",
    "            'tMedicareNumber': 'object',\n",
    "            'tMRN': 'object',\n",
    "            'GroupID': 'object',\n",
    "            'PatientAreaUniqueIdentifier': 'object',\n",
    "            'tFirstGivenName': 'object',\n",
    "            'tSecondGivenName': 'object',\n",
    "            'tSurname': 'object',\n",
    "            'tSex': 'object',\n",
    "            'tDateOfBirth': 'object',\n",
    "            'tURAStreet': 'object',\n",
    "            'tURASuburb': 'object',\n",
    "            'tURAPostcode': 'object',\n",
    "            'tURAState': 'object',\n",
    "            'tIndigenousStatus': 'object',\n",
    "            'tAMO.AHPRA.RegistrationNumberOfTreatingDoctor': 'object',\n",
    "            'tTreatingDoctorName': 'object',\n",
    "            'tHospitalFacilityIdentifier': 'object',\n",
    "            'tCancerDepartmentIdentifier': 'object',\n",
    "            'FacilityType': 'object',\n",
    "            'tDateOfPrimaryDiagnosis': 'object',\n",
    "            'tPrimarySiteOfCancerCode': 'object',\n",
    "            'tCancerBestBasisOfDiagnosis': 'object',\n",
    "            'tLateralityOfPrimaryCancer': 'object',\n",
    "            'tHistopathologicalGrade': 'object',\n",
    "            'tMorphologyOfCancerCode': 'object',\n",
    "            'tTStage': 'object',\n",
    "            'tNStage': 'object',\n",
    "            'tMStage': 'object',\n",
    "            'tTNMStageGroup': 'object',\n",
    "            'DerivedTNMStageGroup': 'object',\n",
    "            'FinalTNMStageGroup': 'object',\n",
    "            'TNMStagingGroupBasis': 'object',\n",
    "            'tPSAScore': 'object',\n",
    "            'tIntentionOfTreatment': 'object',\n",
    "            'tRadiotherapyType': 'object',\n",
    "            'tCourseName': 'object',\n",
    "            'tPriorityCode': 'object',\n",
    "            'tPrescribedDose': 'object',\n",
    "            'tActualDose': 'object',\n",
    "            'tPrescribedFractions': 'object',\n",
    "            'tActualFractions': 'object',\n",
    "            'tDateOfReferralToCancerSpecialist': 'object',\n",
    "            'tReadyForCareDate': 'object',\n",
    "            'tDateOfFirstConsultationWithCancerSpecialist': 'object',\n",
    "            'tDateOfMultidisciplinaryTeamConsultation': 'object',\n",
    "            'tDateOfReferralToPalliativeCare': 'object',\n",
    "            'tPerformanceStatusDate': 'object',\n",
    "            'tPerformanceStatus': 'object',\n",
    "            'tTreatmentStartDate': 'object',\n",
    "            'tTreatmentEndDate': 'object',\n",
    "            'LHDCode': 'object',\n",
    "            'TypeOfBatch': 'object',\n",
    "            'ImportType': 'object',\n",
    "            'CreatedNotificationId': 'object',\n",
    "            'MessageInErrorFile.Batch.Review': 'object',\n",
    "            'ProgER': 'object',\n",
    "            'Progher2': 'object',\n",
    "            'Progher2method': 'object',\n",
    "            'Progher2method2': 'object',\n",
    "            'Progher2status': 'object',\n",
    "            'Progher2status2': 'object',\n",
    "            'ProgPR': 'object',\n",
    "            'ProgSummary': 'object',\n",
    "            'tSource': 'object',\n",
    "            'tLHDName': 'object',\n",
    "            'SectorID': 'object',\n",
    "            'tFacilityName': 'object',\n",
    "            'TreatmentYear': 'object',\n",
    "            'Snapshot': 'object',\n",
    "            'NotificationMethod': 'object',\n",
    "            'AgeAtTreatment': 'object',\n",
    "            'NotificationType': 'object',\n",
    "            'HospitalName': 'object',\n",
    "            'PeerGroup2016': 'object',\n",
    "            'PeerGroupName': 'object',\n",
    "            'HEROIdentifier': 'object',\n",
    "            'StateExcelFile': 'object',\n",
    "            'PatientID.Derived': 'object',\n",
    "            'RecordSource': 'object',\n",
    "            'LoadDateTime': 'object',\n",
    "            'TaskInstanceID': 'object',\n",
    "            'Longitude': 'object',\n",
    "            'Latitude': 'object',\n",
    "            'MeshBlockCode2016': 'object',\n",
    "            'MeshBlockCode2011': 'object',\n",
    "            'CDCode1986': 'object',\n",
    "            'CDCode1991': 'object',\n",
    "            'CDCode1996': 'object',\n",
    "            'CDCode2001': 'object',\n",
    "            'CDCode2006': 'object',\n",
    "            'LGACode2011': 'object',\n",
    "            'LGACode2016': 'object',\n",
    "            'LHDCode2010': 'object',\n",
    "            'SLACode2011': 'object',\n",
    "            'ValidityFlag': 'object',\n",
    "            'Reliability': 'object',\n",
    "            'GIFFlags': 'object',\n",
    "            'AmendedFlag': 'object',\n",
    "            'tReturnAddressLine': 'object',\n",
    "            'tReturnLocalityName': 'object',\n",
    "            'tReturnStateName': 'object',\n",
    "            'tReturnPostCode': 'object',\n",
    "            'AddressID.Derived': 'object',\n",
    "            'AddressID': 'object',\n",
    "            'CaseID.Derived': 'object',\n",
    "            'RecordID': 'object',\n",
    "            'tSiteName': 'object',\n",
    "            'SiteNameBoost': 'object',\n",
    "            'SiteNameLymphNode': 'object',\n",
    "            'tRadiotherapyInformationSystemSiteName': 'object',\n",
    "            'tNotifiableCancer': 'object',\n",
    "            'LastDigit0': 'object',\n",
    "            'LastDigit': 'object',\n",
    "            'First3PrimaryICD10': 'object'}\n",
    "\n",
    "        parse_dates = ['tSiteStartDate']   # the temporal column to be used to split into smaller files\n",
    "\n",
    "        # Open the input file for reading\n",
    "        df = pd.read_csv(inputPath, sep = ',', dtype = dtypes, parse_dates = parse_dates, encoding = 'cp1252')\n",
    "        \n",
    "        # For each continuous numeric column, we need to calculate the mean over the whole dataset, so that we can\n",
    "        # use the mean to add Gaussian noise to a dirty file if all columns in a clean file are null.\n",
    "        # The below column numbers: tPSAScore, tPrescribedDose, tActualDose, tPrescribedFractions, tActualFractions\n",
    "        continuousNumericColumns = [37, 42, 43, 44, 45]\n",
    "        continuousNumericMeans = {};\n",
    "        for errorColumn in continuousNumericColumns:\n",
    "            meanContinuousNumericValue = pd.to_numeric(df.iloc[:, errorColumn]).mean()\n",
    "            # Convert column number to string because dictionary key cannot be numeric\n",
    "            continuousNumericMeans[str(errorColumn)] = meanContinuousNumericValue\n",
    "\n",
    "        generate_clean_files(df, sliceBy, cleanFileNamePrefix, cleanPath)\n",
    "\n",
    "        generate_dirty_files(cleanPath, cleanName, dirtyPath, dirtyName, dtypes, parse_dates, errorMagnitudes,\n",
    "                             typoEveryNthChar, continuousNumericMeans, continuousNumericColumns)\n",
    "    else:\n",
    "        print('Input file \"' + inputPath + '\" does not exist')\n",
    "        \n",
    "    print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7d63e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
